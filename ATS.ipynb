{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec,doc2vec\n",
    "import nltk\n",
    "from quadratic_weighted_kappa import QWK\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "from additional_feature_getter import feature_getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\t##truncnorm generate continuous random numbers in given range\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\t# masked values in x (number_of_samples,time)\n",
    "\t\tself.supports_masking=True\n",
    "\t\t# Specifies number of dimensions to each layer\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "\n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\t\t\t#dimension size single vec/number of samples\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)\n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "fp1=open(\"glove/glove.6B.300d.txt\",\"r\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "VALIDATION_SPLIT=0.20\n",
    "DELTA=20\n",
    "\n",
    "\n",
    "texts=[]\n",
    "labels=[]\n",
    "sentences=[]\n",
    "\n",
    "additional_features=[]\n",
    "\n",
    "originals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range min -  0.0  ; range max -  3.0\n"
     ]
    }
   ],
   "source": [
    "essay_type = '4'\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[6]))\n",
    "fp.close()\n",
    "\n",
    "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences=[]\n",
    "doctovec=[]\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[2])\n",
    "        labels.append((float(temp[6])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
    "        line=temp[2].strip()\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author concludes the story with this because gardens cannot grow with snow on the ground. So she she figurers when all the snow is melted shell try to grow another one. I feel as if she wasnt  happy enough to grow another garden I think she should be more interested in growin it then disappointed.    '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labels appended 1772\n"
     ]
    }
   ],
   "source": [
    "print(\"text labels appended %s\" %len(texts))\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "\n",
    "for i in texts:\n",
    "\tadditional_features.append(feature_getter(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 60, 3, 20.0, 4.1, 4, 246]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additional_features=np.asarray(additional_features)\n",
    "for i in sentences:\n",
    "\ttemp1=np.zeros((1, EMBEDDING_DIM))\n",
    "\tfor w in i:\n",
    "\t\tif(w in glove_emb):\n",
    "\t\t\ttemp1+=glove_emb[w]\n",
    "\ttemp1/=len(i)\n",
    "\tdoctovec.append(temp1.reshape(300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doctovec=np.asarray(doctovec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1772, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(doctovec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4953 unique tokens.\n",
      "(1772, 7) (1772, 500)\n",
      "Shape of data tensor: (1772, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer() #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
    "word_index=tokenizer.word_index #dictionary mapping\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(additional_features.shape,data.shape)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices=np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "doctovec=doctovec[indices]\n",
    "additional_features=additional_features[indices]\n",
    "validation_size=int(VALIDATION_SPLIT*data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=data[:-validation_size]\n",
    "y_train=labels[:-validation_size]\n",
    "doctovec_train=doctovec[:-validation_size]\n",
    "x_val=data[-validation_size:]\n",
    "y_val=labels[-validation_size:]\n",
    "train_ad=additional_features[:-validation_size]\n",
    "val_ad=additional_features[-validation_size:]\n",
    "doctovec_val=doctovec[-validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model=Word2Vec(sentences,min_count=1, sg=1,size=EMBEDDING_DIM)\n",
    "\n",
    "# model.save(\"3_embeddings\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word,i in word_index.items():\n",
    "\tif(i>=len(word_index)):\n",
    "\t\tcontinue\n",
    "\tif word in glove_emb:\n",
    "\t\t\tembedding_matrix[i]=glove_emb[word]\n",
    "\t# if word in model.wv.vocab.keys():\n",
    "\t# \tembedding_matrix[i]=model.wv[word]\n",
    "\n",
    "vocab_size=len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\ttrainable=False)\n",
    "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\ttrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=5, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "\te = Input(name='essay',shape=(maxlen,))\n",
    "\ttrad_feats=Input(shape=(7,))\n",
    "\tdtov=Input(shape=(300,))\n",
    "\tembed = embedding_layer(e)\n",
    "\tside_embed = side_embedding_layer(e)\n",
    "\tlstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "\thidden_states=lstm_layer(embed)\n",
    "\tside_hidden_states=lstm_layer(side_embed)\n",
    "\thtm=Temporal_Mean_Pooling()(hidden_states)\n",
    "\ttensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=lstm_dim)\n",
    "\tpairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "\thidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "\tsigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "\tcoherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "\tco_tm=Concatenate()(coherence[:]+[htm])\n",
    "\tdense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "\tdense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "\tdense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "\tout = Dense(1, activation=\"sigmoid\")(dense)\n",
    "\tmodel = Model(inputs=[e,trad_feats,dtov], outputs=[out])\n",
    "\tadam = Adam(lr=lr, decay=lr_decay)\n",
    "\tmodel.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "sf_1 = SKIPFLOW(lstm_dim=50, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1418 samples, validate on 354 samples\n",
      "Epoch 1/100\n",
      "1418/1418 [==============================] - 9s 6ms/step - loss: 0.0999 - mean_squared_error: 0.0999 - val_loss: 0.0951 - val_mean_squared_error: 0.0951\n",
      "Epoch 2/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0985 - mean_squared_error: 0.0985 - val_loss: 0.0943 - val_mean_squared_error: 0.0943\n",
      "Epoch 3/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0975 - mean_squared_error: 0.0975 - val_loss: 0.0937 - val_mean_squared_error: 0.0937\n",
      "Epoch 4/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0967 - mean_squared_error: 0.0967 - val_loss: 0.0928 - val_mean_squared_error: 0.0928\n",
      "Epoch 5/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0957 - mean_squared_error: 0.0957 - val_loss: 0.0918 - val_mean_squared_error: 0.0918\n",
      "Epoch 6/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0946 - mean_squared_error: 0.0946 - val_loss: 0.0905 - val_mean_squared_error: 0.0905\n",
      "Epoch 7/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0934 - mean_squared_error: 0.0934 - val_loss: 0.0892 - val_mean_squared_error: 0.0892\n",
      "Epoch 8/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0921 - mean_squared_error: 0.0921 - val_loss: 0.0879 - val_mean_squared_error: 0.0879\n",
      "Epoch 9/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.0865 - val_mean_squared_error: 0.0865\n",
      "Epoch 10/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0891 - mean_squared_error: 0.0891 - val_loss: 0.0850 - val_mean_squared_error: 0.0850\n",
      "Epoch 11/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0873 - mean_squared_error: 0.0873 - val_loss: 0.0833 - val_mean_squared_error: 0.0833\n",
      "Epoch 12/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0856 - mean_squared_error: 0.0856 - val_loss: 0.0815 - val_mean_squared_error: 0.0815\n",
      "Epoch 13/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0836 - mean_squared_error: 0.0836 - val_loss: 0.0794 - val_mean_squared_error: 0.0794\n",
      "Epoch 14/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0815 - mean_squared_error: 0.0815 - val_loss: 0.0771 - val_mean_squared_error: 0.0771\n",
      "Epoch 15/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.0747 - val_mean_squared_error: 0.0747\n",
      "Epoch 16/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0769 - mean_squared_error: 0.0769 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
      "Epoch 17/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0744 - mean_squared_error: 0.0744 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "Epoch 18/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0714 - mean_squared_error: 0.0714 - val_loss: 0.0667 - val_mean_squared_error: 0.0667\n",
      "Epoch 19/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0684 - mean_squared_error: 0.0684 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 20/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0652 - mean_squared_error: 0.0652 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 21/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0619 - mean_squared_error: 0.0619 - val_loss: 0.0576 - val_mean_squared_error: 0.0576\n",
      "Epoch 22/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0588 - mean_squared_error: 0.0588 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 23/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0556 - mean_squared_error: 0.0556 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 24/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0523 - mean_squared_error: 0.0523 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 25/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0498 - mean_squared_error: 0.0498 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 26/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 27/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 28/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 29/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 30/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 31/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 32/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 33/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 34/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 35/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 36/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 37/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 38/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 39/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 40/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 41/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 42/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0333 - val_mean_squared_error: 0.0333\n",
      "Epoch 43/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 44/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0329 - val_mean_squared_error: 0.0329\n",
      "Epoch 45/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 46/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0326 - val_mean_squared_error: 0.0326\n",
      "Epoch 47/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0329 - val_mean_squared_error: 0.0329\n",
      "Epoch 48/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0324 - val_mean_squared_error: 0.0324\n",
      "Epoch 49/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0324 - val_mean_squared_error: 0.0324\n",
      "Epoch 50/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0322 - val_mean_squared_error: 0.0322\n",
      "Epoch 51/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0322 - val_mean_squared_error: 0.0322\n",
      "Epoch 52/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0320 - val_mean_squared_error: 0.0320\n",
      "Epoch 53/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0320 - val_mean_squared_error: 0.0320\n",
      "Epoch 54/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0319 - val_mean_squared_error: 0.0319\n",
      "Epoch 55/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0316 - val_mean_squared_error: 0.0316\n",
      "Epoch 56/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0315 - val_mean_squared_error: 0.0315\n",
      "Epoch 57/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0314 - val_mean_squared_error: 0.0314\n",
      "Epoch 58/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0314 - val_mean_squared_error: 0.0314\n",
      "Epoch 59/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0313 - val_mean_squared_error: 0.0313\n",
      "Epoch 60/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0312 - val_mean_squared_error: 0.0312\n",
      "Epoch 61/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0311 - val_mean_squared_error: 0.0311\n",
      "Epoch 62/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0310 - val_mean_squared_error: 0.0310\n",
      "Epoch 63/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0309 - val_mean_squared_error: 0.0309\n",
      "Epoch 64/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0308 - val_mean_squared_error: 0.0308\n",
      "Epoch 65/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0308 - val_mean_squared_error: 0.0308\n",
      "Epoch 66/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0308 - val_mean_squared_error: 0.0308\n",
      "Epoch 67/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0309 - val_mean_squared_error: 0.0309\n",
      "Epoch 68/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0308 - val_mean_squared_error: 0.0308\n",
      "Epoch 69/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 70/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 71/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 72/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0303 - val_mean_squared_error: 0.0303\n",
      "Epoch 73/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0302 - val_mean_squared_error: 0.0302\n",
      "Epoch 74/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0300 - val_mean_squared_error: 0.0300\n",
      "Epoch 75/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0300 - val_mean_squared_error: 0.0300\n",
      "Epoch 76/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
      "Epoch 77/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0301 - val_mean_squared_error: 0.0301\n",
      "Epoch 78/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
      "Epoch 79/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
      "Epoch 80/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0301 - val_mean_squared_error: 0.0301\n",
      "Epoch 81/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
      "Epoch 82/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
      "Epoch 83/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
      "Epoch 84/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0301 - val_mean_squared_error: 0.0301\n",
      "Epoch 85/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
      "Epoch 86/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
      "Epoch 87/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
      "Epoch 88/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0296 - val_mean_squared_error: 0.0296\n",
      "Epoch 89/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
      "Epoch 90/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0296 - val_mean_squared_error: 0.0296\n",
      "Epoch 91/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
      "Epoch 92/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
      "Epoch 93/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
      "Epoch 94/100\n",
      "1418/1418 [==============================] - 3s 2ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "# epochs = 1000\n",
    "hist = sf_1.fit([x_train,train_ad,doctovec_train], y_train, batch_size=1024, epochs=epochs,\n",
    "                validation_data=([x_val,val_ad,doctovec_val], y_val), callbacks=[earlystopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=sf_1.predict([x_val,val_ad,doctovec_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(354).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7902661772781133\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1.save('4_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37.655647],\n",
       "       [37.61193 ],\n",
       "       [43.24997 ],\n",
       "       [40.21272 ],\n",
       "       [38.533585],\n",
       "       [36.21241 ],\n",
       "       [36.59495 ],\n",
       "       [35.90928 ],\n",
       "       [41.424652],\n",
       "       [38.39617 ],\n",
       "       [30.6541  ],\n",
       "       [33.754498],\n",
       "       [29.771978],\n",
       "       [38.186462],\n",
       "       [35.046486],\n",
       "       [31.218304],\n",
       "       [40.664257],\n",
       "       [41.2769  ],\n",
       "       [40.152946],\n",
       "       [34.431534],\n",
       "       [37.73353 ],\n",
       "       [40.108177],\n",
       "       [40.490265],\n",
       "       [35.33967 ],\n",
       "       [36.986275],\n",
       "       [36.98564 ],\n",
       "       [35.761414],\n",
       "       [38.322144],\n",
       "       [34.203346],\n",
       "       [45.936516],\n",
       "       [41.50494 ],\n",
       "       [34.36251 ],\n",
       "       [39.254936],\n",
       "       [42.239418],\n",
       "       [38.22659 ],\n",
       "       [37.256927],\n",
       "       [40.80839 ],\n",
       "       [29.887756],\n",
       "       [38.07134 ],\n",
       "       [37.655693],\n",
       "       [37.428432],\n",
       "       [37.049194],\n",
       "       [34.822044],\n",
       "       [42.396366],\n",
       "       [39.12339 ],\n",
       "       [37.936356],\n",
       "       [39.9822  ],\n",
       "       [38.402573],\n",
       "       [29.93917 ],\n",
       "       [41.666664],\n",
       "       [36.278633],\n",
       "       [40.06159 ],\n",
       "       [37.182503],\n",
       "       [33.913197],\n",
       "       [34.678616],\n",
       "       [42.36648 ],\n",
       "       [38.025284],\n",
       "       [39.955612],\n",
       "       [36.742687],\n",
       "       [40.350155],\n",
       "       [40.54757 ],\n",
       "       [36.031   ],\n",
       "       [37.7251  ],\n",
       "       [36.039917],\n",
       "       [30.872446],\n",
       "       [40.04986 ],\n",
       "       [37.00627 ],\n",
       "       [34.810375],\n",
       "       [40.777233],\n",
       "       [35.043785],\n",
       "       [37.020687],\n",
       "       [37.10832 ],\n",
       "       [33.64111 ],\n",
       "       [34.25443 ],\n",
       "       [44.208286],\n",
       "       [34.352356],\n",
       "       [40.468483],\n",
       "       [29.741077],\n",
       "       [33.594414],\n",
       "       [36.45014 ],\n",
       "       [43.265392],\n",
       "       [34.566402],\n",
       "       [45.458076],\n",
       "       [36.1565  ],\n",
       "       [36.343216],\n",
       "       [27.04903 ],\n",
       "       [40.78064 ],\n",
       "       [35.887962],\n",
       "       [41.136414],\n",
       "       [31.83655 ],\n",
       "       [33.813194],\n",
       "       [27.78556 ],\n",
       "       [30.899845],\n",
       "       [39.303535],\n",
       "       [37.385834],\n",
       "       [43.11172 ],\n",
       "       [43.17352 ],\n",
       "       [36.940033],\n",
       "       [42.13676 ],\n",
       "       [43.89941 ],\n",
       "       [35.108475],\n",
       "       [37.0643  ],\n",
       "       [35.539665],\n",
       "       [42.145287],\n",
       "       [34.834442],\n",
       "       [34.096535],\n",
       "       [40.087296],\n",
       "       [29.773222],\n",
       "       [41.12883 ],\n",
       "       [39.61929 ],\n",
       "       [41.142517],\n",
       "       [33.03411 ],\n",
       "       [31.285269],\n",
       "       [36.734413],\n",
       "       [32.405525],\n",
       "       [32.758953],\n",
       "       [37.35199 ],\n",
       "       [37.57493 ],\n",
       "       [30.659874],\n",
       "       [41.426598],\n",
       "       [31.287184],\n",
       "       [33.560753],\n",
       "       [38.668648],\n",
       "       [37.562866],\n",
       "       [34.022118],\n",
       "       [36.08467 ],\n",
       "       [36.14338 ],\n",
       "       [42.390575],\n",
       "       [43.473473],\n",
       "       [38.39348 ],\n",
       "       [38.37836 ],\n",
       "       [35.860954],\n",
       "       [37.47364 ],\n",
       "       [40.722473],\n",
       "       [28.525328],\n",
       "       [30.853294],\n",
       "       [36.44873 ],\n",
       "       [26.167152],\n",
       "       [36.440697],\n",
       "       [33.85443 ],\n",
       "       [32.678032],\n",
       "       [35.841606],\n",
       "       [34.45412 ],\n",
       "       [36.074917]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred*(range_max-range_min)+range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 38,\n",
       " 43,\n",
       " 40,\n",
       " 39,\n",
       " 36,\n",
       " 37,\n",
       " 36,\n",
       " 41,\n",
       " 38,\n",
       " 31,\n",
       " 34,\n",
       " 30,\n",
       " 38,\n",
       " 35,\n",
       " 31,\n",
       " 41,\n",
       " 41,\n",
       " 40,\n",
       " 34,\n",
       " 38,\n",
       " 40,\n",
       " 40,\n",
       " 35,\n",
       " 37,\n",
       " 37,\n",
       " 36,\n",
       " 38,\n",
       " 34,\n",
       " 46,\n",
       " 42,\n",
       " 34,\n",
       " 39,\n",
       " 42,\n",
       " 38,\n",
       " 37,\n",
       " 41,\n",
       " 30,\n",
       " 38,\n",
       " 38,\n",
       " 37,\n",
       " 37,\n",
       " 35,\n",
       " 42,\n",
       " 39,\n",
       " 38,\n",
       " 40,\n",
       " 38,\n",
       " 30,\n",
       " 42,\n",
       " 36,\n",
       " 40,\n",
       " 37,\n",
       " 34,\n",
       " 35,\n",
       " 42,\n",
       " 38,\n",
       " 40,\n",
       " 37,\n",
       " 40,\n",
       " 41,\n",
       " 36,\n",
       " 38,\n",
       " 36,\n",
       " 31,\n",
       " 40,\n",
       " 37,\n",
       " 35,\n",
       " 41,\n",
       " 35,\n",
       " 37,\n",
       " 37,\n",
       " 34,\n",
       " 34,\n",
       " 44,\n",
       " 34,\n",
       " 40,\n",
       " 30,\n",
       " 34,\n",
       " 36,\n",
       " 43,\n",
       " 35,\n",
       " 45,\n",
       " 36,\n",
       " 36,\n",
       " 27,\n",
       " 41,\n",
       " 36,\n",
       " 41,\n",
       " 32,\n",
       " 34,\n",
       " 28,\n",
       " 31,\n",
       " 39,\n",
       " 37,\n",
       " 43,\n",
       " 43,\n",
       " 37,\n",
       " 42,\n",
       " 44,\n",
       " 35,\n",
       " 37,\n",
       " 36,\n",
       " 42,\n",
       " 35,\n",
       " 34,\n",
       " 40,\n",
       " 30,\n",
       " 41,\n",
       " 40,\n",
       " 41,\n",
       " 33,\n",
       " 31,\n",
       " 37,\n",
       " 32,\n",
       " 33,\n",
       " 37,\n",
       " 38,\n",
       " 31,\n",
       " 41,\n",
       " 31,\n",
       " 34,\n",
       " 39,\n",
       " 38,\n",
       " 34,\n",
       " 36,\n",
       " 36,\n",
       " 42,\n",
       " 43,\n",
       " 38,\n",
       " 38,\n",
       " 36,\n",
       " 37,\n",
       " 41,\n",
       " 29,\n",
       " 31,\n",
       " 36,\n",
       " 26,\n",
       " 36,\n",
       " 34,\n",
       " 33,\n",
       " 36,\n",
       " 34,\n",
       " 36]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 45,\n",
       " 42,\n",
       " 45,\n",
       " 45,\n",
       " 36,\n",
       " 40,\n",
       " 35,\n",
       " 44,\n",
       " 40,\n",
       " 10,\n",
       " 30,\n",
       " 30,\n",
       " 43,\n",
       " 38,\n",
       " 20,\n",
       " 40,\n",
       " 44,\n",
       " 36,\n",
       " 36,\n",
       " 37,\n",
       " 40,\n",
       " 40,\n",
       " 35,\n",
       " 40,\n",
       " 36,\n",
       " 35,\n",
       " 35,\n",
       " 32,\n",
       " 44,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 42,\n",
       " 36,\n",
       " 40,\n",
       " 30,\n",
       " 44,\n",
       " 40,\n",
       " 37,\n",
       " 36,\n",
       " 33,\n",
       " 41,\n",
       " 40,\n",
       " 40,\n",
       " 45,\n",
       " 37,\n",
       " 32,\n",
       " 44,\n",
       " 50,\n",
       " 34,\n",
       " 34,\n",
       " 38,\n",
       " 42,\n",
       " 45,\n",
       " 40,\n",
       " 39,\n",
       " 36,\n",
       " 35,\n",
       " 34,\n",
       " 30,\n",
       " 43,\n",
       " 42,\n",
       " 37,\n",
       " 36,\n",
       " 31,\n",
       " 38,\n",
       " 42,\n",
       " 36,\n",
       " 32,\n",
       " 40,\n",
       " 40,\n",
       " 35,\n",
       " 45,\n",
       " 40,\n",
       " 45,\n",
       " 28,\n",
       " 37,\n",
       " 36,\n",
       " 40,\n",
       " 35,\n",
       " 46,\n",
       " 40,\n",
       " 34,\n",
       " 27,\n",
       " 40,\n",
       " 37,\n",
       " 42,\n",
       " 33,\n",
       " 34,\n",
       " 20,\n",
       " 26,\n",
       " 38,\n",
       " 28,\n",
       " 36,\n",
       " 42,\n",
       " 40,\n",
       " 45,\n",
       " 50,\n",
       " 30,\n",
       " 40,\n",
       " 34,\n",
       " 35,\n",
       " 38,\n",
       " 37,\n",
       " 40,\n",
       " 31,\n",
       " 36,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 30,\n",
       " 40,\n",
       " 40,\n",
       " 20,\n",
       " 34,\n",
       " 40,\n",
       " 33,\n",
       " 32,\n",
       " 44,\n",
       " 36,\n",
       " 41,\n",
       " 37,\n",
       " 31,\n",
       " 30,\n",
       " 37,\n",
       " 50,\n",
       " 34,\n",
       " 40,\n",
       " 47,\n",
       " 37,\n",
       " 36,\n",
       " 38,\n",
       " 28,\n",
       " 25,\n",
       " 40,\n",
       " 27,\n",
       " 45,\n",
       " 33,\n",
       " 36,\n",
       " 32,\n",
       " 30,\n",
       " 34]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-53dfd81e5f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(sf_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1.save_weights('4_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
